# ğŸ§­ Smart GIKI Navigator â€“ Vision & Voice Guided Campus Mapping  
## ğŸ¯ A Location-Aware Navigation Demo Without GPS  
**Built with:** MobileNetV2 â€¢ Speech Recognition â€¢ Google Maps API  
**License:** MIT | Python | Streamlit | TorchVision | gTTS  

---

ğŸ¥ **Your camera becomes your compass.**  
ğŸ§  This AI-powered app locates you using a photo, takes your voice as destination input, and gives real-time walking directions â€” all without relying on GPS.

---

## ğŸ’¡ Project Overview & Motivation  

How do you navigate when GPS isnâ€™t available â€” like deep inside campuses or academic blocks?

**Smart GIKI Navigator** tackles that by combining computer vision, natural language processing, and real-world map APIs. This demo application was developed for the CV & NLP course and is capable of:

- Detecting your location visually via a **MobileNetV2 image classifier**
- Taking **voice input** to extract your destination
- Generating **step-by-step walking directions** using Google Maps
- Speaking those steps aloud using **gTTS (text-to-speech)**

Whether youâ€™re a student, visitor, or just exploring campus, this tool acts like your personal AR navigation assistant.

---

## âœ¨ Key Features  

- ğŸ” **Visual Location Detection**  
  Upload a building photo â†’ AI classifies your current location.

- ğŸ™ï¸ **Voice Navigation**  
  Speak your destination â†’ system extracts & understands the command.

- ğŸ—ºï¸ **Google Maps Step-by-Step Directions**  
  Uses coordinates + Maps API to calculate walking route.

- ğŸ”Š **Audio Directions**  
  Each direction is spoken aloud using gTTS.

- ğŸ–¥ï¸ **Streamlit Web Interface**  
  Fully interactive â€” no code or CLI required.

---

## ğŸš€ How It Works  

1. **Upload an Image**  
   â†’ Model classifies which GIKI building you're in.

2. **Speak or Type Your Destination**  
   â†’ Voice command is parsed using speech recognition + keyword mapping.

3. **Generate Directions**  
   â†’ API fetches walking instructions from your current location to the destination.

4. **Listen & Follow**  
   â†’ Steps are displayed *and* spoken in real-time.

---

## ğŸ“ Project Structure  

Smart-GIKI-Navigator/
â”œâ”€â”€ app-base.py # Streamlit main app
â”œâ”€â”€ train_classifier.py # Training script for MobileNetV2
â”œâ”€â”€ .env # Google Maps API key (DO NOT COMMIT)
â”œâ”€â”€ static/
â”‚ â””â”€â”€ audio/ # Voice instruction files
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ CV_utils.py # Visual location classifier
â”‚ â”œâ”€â”€ voice_utils.py # Voice input and TTS
â”‚ â””â”€â”€ map_utils.py # Google Maps API interface
â”œâ”€â”€ models/
â”‚ â””â”€â”€ building_classifier.pth # Saved PyTorch model
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ LICENSE # MIT License
â””â”€â”€ README.md # This file

yaml
Copy
Edit

---

## ğŸ”’ API Keys & Model Notes  

- **Google Maps API**  
  Store your API key in a `.env` file:
  ```env
  GOOGLE_MAPS_API_KEY=YOUR_API_KEY_HERE
âš ï¸ Make sure to add .env to .gitignore before pushing to GitHub!

Model Checkpoint
The file building_classifier.pth stores the trained MobileNetV2 model that detects 12 different GIKI buildings.

ğŸ› ï¸ Setup & Installation
1. Clone the Repository
bash
Copy
Edit
git clone https://github.com/yourusername/Smart-GIKI-Navigator.git
cd Smart-GIKI-Navigator
2. Create Virtual Environment (Recommended)
bash
Copy
Edit
python -m venv venv
# Activate:
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
3. Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
4. Add Your API Key
Create a file named .env in the root directory:

env
Copy
Edit
GOOGLE_MAPS_API_KEY=YOUR_API_KEY_HERE
5. Run the App
bash
Copy
Edit
streamlit run app-base.py
ğŸ§ª Train Your Own Model
To retrain the MobileNetV2 image classifier:

Organize images into class-labeled folders like:
data/Library/, data/FCSE/, etc.

Update the DATA_DIR path inside train_classifier.py

Run:

bash
Copy
Edit
python train_classifier.py
New model will be saved at models/building_classifier.pth

ğŸ™‹â€â™‚ï¸ Contributing
Pull requests are welcome!
To contribute:

Fork this repository

Create a feature branch

Make your changes

Submit a pull request with a clear explanation

ğŸ“œ License
This project is released under the MIT License.
Feel free to reuse, remix, and share â€” just credit the authors.

ğŸ‘¥ Credits
Developed by Aaiz Mohsin
ğŸ“ BS Artificial Intelligence â€“ GIKI (2022â€“2026)
ğŸ§  Final Project for CV & NLP (6th Semester)

Huge thanks to faculty, teammates, and every friend whose photo made this demo possible!

ğŸŒ Letâ€™s Connect
ğŸ’» GitHub â€“ Aaiz-Am17

ğŸ’¬ Suggestions? Feedback? Drop a â­ if you liked it!
